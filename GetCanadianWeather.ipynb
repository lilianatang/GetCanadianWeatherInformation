{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbaab475-3a70-444c-83d6-c9ac10f55c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ed5cda-fc80-4652-81c5-1e4c97421b4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def download_city_weather(station_id, station_name, city, start_year=1990, end_year=2025):\n",
    "    \"\"\"\n",
    "    Downloads daily weather data for Edmonton from Environment Canada.\n",
    "    Handles the station transition from City Centre to Blatchford.\n",
    "    \"\"\"\n",
    "    \n",
    "    # URL template for Environment Canada's bulk download\n",
    "    base_url = \"https://climate.weather.gc.ca/climate_data/bulk_data_e.html\"\n",
    "    \n",
    "    # Storage for annual dataframes\n",
    "    all_data = []\n",
    "\n",
    "    print(f\"Starting download for {station_name} weather data ({start_year}-{end_year})...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "\n",
    "        params = {\n",
    "            'format': 'csv',\n",
    "            'stationID': station_id,\n",
    "            'Year': year,\n",
    "            'Month': 1,      # Month/Day required by API but ignored for daily/bulk dl\n",
    "            'Day': 1,\n",
    "            'timeframe': 2,  # 2 = Daily data\n",
    "            'submit': 'Download Data'\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Fetch the data\n",
    "            response = requests.get(base_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Read CSV from memory\n",
    "            # specific string decode is safe for these files\n",
    "            content = response.content.decode('utf-8')\n",
    "            \n",
    "            # Environment Canada CSVs often have metadata headers; pandas handles this well\n",
    "            # if we simply read it. Sometimes dates parse better if specified.\n",
    "            df = pd.read_csv(io.StringIO(content))\n",
    "            \n",
    "            # Basic cleanup: Keep only rows where 'Date/Time' is actual data\n",
    "            if 'Date/Time' in df.columns:\n",
    "                df['Date/Time'] = pd.to_datetime(df['Date/Time'])\n",
    "                \n",
    "                # Add a column to track which station this came from (optional but helpful)\n",
    "                df['Source_Station_ID'] = station_id\n",
    "                df['Source_Station_Name'] = station_name\n",
    "                df['City'] = city\n",
    "                all_data.append(df)\n",
    "                print(f\"✓ {year} downloaded ({station_name})\")\n",
    "            else:\n",
    "                print(f\"⚠ {year} downloaded but format seemed empty or incorrect.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to download {year}: {e}\")\n",
    "        \n",
    "        # Be polite to the server\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Combine all years into one DataFrame\n",
    "    if all_data:\n",
    "        full_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Sort by date to ensure clean chronology\n",
    "        full_df.sort_values('Date/Time', inplace=True)\n",
    "        spark.sql(\"CREATE TABLE IF NOT EXISTS main.ext.weather\")\n",
    "        #print(\"Original DataFrame:\")\n",
    "        #display(full_df)\n",
    "        #print(list(full_df.columns))\n",
    "        full_df = full_df.rename(columns={'Longitude (x)':'Longitude', 'Latitude (y)':'Latitude', 'Station Name': 'StationName', 'Climate ID':'ClimateID', 'Date/Time':'DateTime', 'Data Quality':'DataQuality', 'Max Temp (°C)':'MaxTemp', 'Max Temp Flag':'MaxTempFlag', 'Min Temp (°C)':'MinTemp', 'Min Temp Flag':'MinTempFlag', 'Mean Temp (°C)':'MeanTemp', 'Mean Temp Flag':'MeanTempFlag', 'Heat Deg Days (°C)':'HeatDegDays', 'Heat Deg Days Flag':'HeatDegDaysFlag', 'Cool Deg Days (°C)':'CoolDegDays', 'Cool Deg Days Flag':'CoolDegDaysFlag', 'Total Rain (mm)':'TotalRain', 'Total Rain Flag':'TotalRainFlag', 'Total Snow (cm)':'TotalSnow', 'Total Snow Flag':'TotalSnowFlag', 'Total Precip (mm)':'TotalPrecip', 'Total Precip Flag':'TotalPrecipFlag', 'Snow on Grnd (cm)':'SnowonGrnd', 'Snow on Grnd Flag':'SnowonGrndFlag', 'Dir of Max Gust (10s deg)':'DirofMaxGust', 'Dir of Max Gust Flag':'DirofMaxGustFlag', 'Spd of Max Gust (km/h)':'SpdofMaxGust', 'Spd of Max Gust Flag':'SpdofMaxGustFlag'})\n",
    "        full_df['MaxTempFlag'] = full_df['MaxTempFlag'].astype(str)\n",
    "        full_df['MinTempFlag'] = full_df['MinTempFlag'].astype(str)\n",
    "        full_df['MeanTempFlag'] = full_df['MeanTempFlag'].astype(str)\n",
    "        full_df['DataQuality'] = full_df['DataQuality'].astype(float)\n",
    "        full_df['TotalRainFlag'] = full_df['TotalRainFlag'].astype(str)\n",
    "        full_df['HeatDegDaysFlag'] = full_df['HeatDegDaysFlag'].astype(str)\n",
    "        full_df['ClimateID'] = pd.to_numeric(full_df['ClimateID'], errors='coerce').astype('Int64')\n",
    "        full_df['DirofMaxGustFlag'] = full_df['DirofMaxGustFlag'].astype(float)\n",
    "        full_df['CoolDegDaysFlag'] = full_df['CoolDegDaysFlag'].astype(str)\n",
    "        full_df['TotalPrecipFlag'] = full_df['TotalPrecipFlag'].astype(str)\n",
    "        full_df['TotalSnowFlag'] = full_df['TotalSnowFlag'].astype(float)\n",
    "        spark_df = spark.createDataFrame(full_df)\n",
    "        spark_df.write.format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(\"main.ext.weather\")\n",
    "        # return full_df\n",
    "    else:\n",
    "        # return pd.DataFrame()\n",
    "        print(\"No more data to download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e787dfe6-1a81-4b4f-a18c-72c950888730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "\n",
    "START_YEAR = 1990\n",
    "END_YEAR = 2025\n",
    "\n",
    "# Official EC Inventory URL\n",
    "INVENTORY_URL = \"https://collaboration.cmc.ec.gc.ca/cmc/climate/Get_More_Data_Plus_de_donnees/Station%20Inventory%20EN.csv\"\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees).\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # Haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "def get_station_inventory():\n",
    "    print(\"Downloading official Station Inventory...\")\n",
    "    try:\n",
    "        response = requests.get(INVENTORY_URL, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        # Skip top 3 metadata rows\n",
    "        df = pd.read_csv(io.StringIO(response.content.decode('utf-8', errors='ignore')), skiprows=3)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading inventory: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def find_nearest_station(target_lat, target_lon, inventory_df):\n",
    "    \"\"\"\n",
    "    Finds the station in the inventory closest to the target lat/lon\n",
    "    that has data within the requested year range.\n",
    "    \"\"\"\n",
    "    if pd.isna(target_lat) or pd.isna(target_lon):\n",
    "        return None, \"Invalid Coordinates\", None\n",
    "\n",
    "    # Filter inventory to stations with valid lat/lon and overlap with our years\n",
    "    # Ensure columns are numeric\n",
    "    inv = inventory_df.copy()\n",
    "    inv['DLY First Year'] = pd.to_numeric(inv['DLY First Year'], errors='coerce').fillna(9999)\n",
    "    inv['DLY Last Year'] = pd.to_numeric(inv['DLY Last Year'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Filter for time overlap (Station must have started before End Year and ended after Start Year)\n",
    "    active_stations = inv[\n",
    "        (inv['DLY First Year'] <= END_YEAR) & \n",
    "        (inv['DLY Last Year'] >= START_YEAR)\n",
    "    ].copy()\n",
    "\n",
    "    if active_stations.empty:\n",
    "        return None, \"No active stations in time range\", None\n",
    "\n",
    "    # Calculate distance to all active stations\n",
    "    # Note: Column names in inventory are usually \"Latitude (Decimal Degrees)\" and \"Longitude (Decimal Degrees)\"\n",
    "    lat_col = [c for c in inv.columns if \"Latitude\" in c and \"Decimal\" in c][0]\n",
    "    lon_col = [c for c in inv.columns if \"Longitude\" in c and \"Decimal\" in c][0]\n",
    "\n",
    "    def get_dist(row):\n",
    "        return haversine(target_lon, target_lat, row[lon_col], row[lat_col])\n",
    "\n",
    "    active_stations['distance_km'] = active_stations.apply(get_dist, axis=1)\n",
    "    \n",
    "    # Sort by distance\n",
    "    nearest = active_stations.sort_values('distance_km').iloc[0]\n",
    "    \n",
    "    return nearest['Station ID'], nearest['Name'], nearest['distance_km']\n",
    "\n",
    "# --- Main Execution ---\n",
    "def save_weather_data():\n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        df = spark.sql(\"SQL STATEMENT TO PULL DATA THAT STORES A LIST OF CITIES YOU WOULD LIKE TO GET WEATHER INFORMATION FOR\").toPandas()\n",
    "        print(f\"Loaded {len(df)} locations.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load input file: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Load Inventory\n",
    "    inventory = get_station_inventory()\n",
    "    if inventory.empty:\n",
    "        print(\"CRITICAL: Inventory download failed.\")\n",
    "        exit()\n",
    "\n",
    "    # 3. Initialize Geocoder\n",
    "    geolocator = Nominatim(user_agent=\"weather_matcher_v1\")\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    print(\"Starting Geocoding and Matching (this takes time to be polite to the server)...\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        location_query = row['County']\n",
    "        print(\"City currently being processed\")\n",
    "        print(location_query)\n",
    "        print(f\"Processing {index + 1}/{len(df)}: {location_query}\")\n",
    "        \n",
    "        try:\n",
    "            # Step A: Geocode\n",
    "            # We append \"Canada\" to ensure we don't get a \"Foothills\" in the USA\n",
    "            query_str = f\"{location_query}, Canada\"\n",
    "            location = geolocator.geocode(query_str, timeout=10)\n",
    "            \n",
    "            if location:\n",
    "                lat, lon = location.latitude, location.longitude\n",
    "                \n",
    "                # Step B: Find Nearest Station\n",
    "                st_id, st_name, dist = find_nearest_station(lat, lon, inventory)\n",
    "                download_city_weather(st_id, st_name, location_query, start_year=1990, end_year=2025)\n",
    "                results.append({\n",
    "                    'Original_Query': location_query,\n",
    "                    'Geocoded_Address': location.address,\n",
    "                    'Lat': lat,\n",
    "                    'Lon': lon,\n",
    "                    'Nearest_Station_ID': st_id,\n",
    "                    'Nearest_Station_Name': st_name,\n",
    "                    'Distance_KM': round(dist, 2)\n",
    "                })\n",
    "                print(f\"  -> Match: {st_name} ({round(dist, 2)} km away)\")\n",
    "            else:\n",
    "                print(\"  -> Could not geocode location.\")\n",
    "                results.append({'Original_Query': location_query, 'Nearest_Station_ID': None, 'Status': 'Geocode Failed'})\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  -> Error: {e}\")\n",
    "            results.append({'Original_Query': location_query, 'Nearest_Station_ID': None, 'Status': 'Error'})\n",
    "        \n",
    "        # Rate limit to respect Nominatim's usage policy (1 second delay)\n",
    "        time.sleep(1)\n",
    "\n",
    "    # 4. Save Results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f64746e-8bb6-474e-8998-fc078487beb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_weather_data()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GetCanadianWeather",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
